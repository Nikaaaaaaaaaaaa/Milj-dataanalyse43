{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef03b76",
   "metadata": {},
   "source": [
    "# Miljødataanalyse - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fb84f",
   "metadata": {},
   "source": [
    "Mappe Del 1 Spørsmål\n",
    "/Oppgave 2\n",
    "1. Hvilke åpne datakilder er identifisert som relevante for miljødata, og hva er kriteriene (f.eks. kildeautoritet, datakvalitet, tilgjengelighet, brukervennlighet osv.) for å vurdere deres pålitelighet og kvalitet?\n",
    "    - Vi har valgt å bruke enten Openweathermap sin API for å samle inn miljø-data, samt historisk data, eller open-meteo sine API-er. Noen API-er fra Openweathermap er utilgjengelige uten å betale. \n",
    "    - For å vurdere deres pålitelighet og kvalitet har vi sett på hvilke kilder desse sidene bruker for å samle sin informasjon. Videre har vi forsøkt å bruke real-time tester for å se om de samsvarer med områdene vi søker på i virkeligheten. Dette var videre lagt fram som et forslag av Professor Rouhani.\n",
    "2. Hvilke teknikker (f.eks. håndtering av CSV-filer, JSON-data) er valgt å bruke for å lese inn dataene, og hvordan påvirker disse valgene datakvaliteten og prosessen videre?\n",
    "    - API-ene fra Openweathermap bruker JSON format, men jeg er usikker på om open-meteo gjør det samme. Deres data kunne bli lastet ned fra terminal.\n",
    "    - Vi har valgt å bruke Visual Studio Code for å arbeide med jupyter-book teknologi\n",
    "3. Dersom det er brukt API-er, hvilke spesifikke API-er er valgt å bruke, og hva er de viktigste dataene som kan hentes fra disse kildene?\n",
    "    - API-er fra Openweathermap som omhandler trender innenfor hvordan været har endret seg over tiden. Dette ligger sterkt med tanke på økende temperatur og sterkere vindforhold.\n",
    "    - API-er fra Open-meteo vier og trender innenfor hvordan været har endret seg over tid. De har og en stor mengde med mulig statistik å kalle på. Fra regn, til temperatur, til jord foktighet. \n",
    "\n",
    "/Oppgave 3\n",
    "1. Hvilke metoder vil du bruke for å identifisere og håndtere manglende verdier i datasettet?\n",
    "    - For å kunne finne manglende verdier så vil dette kunne løses primært sett på to måter Først gjør en en test for å se om det finnes mangler. Om det mangler for mange verdier, så fjerner vi dem. Alternativt så kan vi fylle den inn selv, via flere forskjeller metoder. Eksempeltvis kan en gjøre det via en enkel count funksjon, alternativt ved å finne median-verdiene for de verdiene som mangler, basert på hva som er før og etter. \n",
    "2. Kan du gi et eksempel på hvordan du vil bruke list comprehensions for å manipulere dataene?\n",
    "    -  Et eksempel som gitt over kan være via grafer. Når en først finner en mangel, så vil det umiddelbart være tre muligheter. De vil så være basert på fillna, hvor en har en forward fill som gir deg samme verdi som den siste nevnte før det var mangel. Den andre versjonen er en backwards fill hvor den så går til neste tall som blir git og fyller den inn baklengs. Den tredje versjonen er median fill, hvor en bruker en tar median-verdi for av hva som kommer etter og før de manglende verdiene for å gi en overgang.\n",
    "3. Hvordan kan Pandas SQL (sqldf) forbedre datamanipuleringen sammenlignet med tradisjonelle Pandas-operasjoner?\n",
    "    - Ved større datasett så vil Pandas SQL sørge for en mer nøyaktig framstilling for videre analyse.\n",
    "4. Hvilke spesifikke uregelmessigheter i dataene forventer du å møte, og hvordan planlegger du å håndtere dem?\n",
    "    - I denne oppgaven planlegges det å møte manglende verdier, hvorav målet vil være å kunne finne ut hvor mange verdier som mangler av det totale datasettet. Om det skulle være for mange verdier, så vil disse droppes for å kunne unngå unøyaktigheter. Er de en påpasselig mengde, så vil vi gjøre en median-fill for å kunne sørge for en så gjennsnittlig måling som mulig. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddb19c",
   "metadata": {},
   "source": [
    "Mappe Del 2 Spørsmål"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8fbd4",
   "metadata": {},
   "source": [
    "/Oppgave 4\n",
    "1. Hvordan kan du bruke NumPy og Pandas til å beregne gjennomsnitt, median og standardavvik for de innsamlede dataene, og hvorfor er disse statistiske målene viktige?\n",
    "    - Numpy og Pandas er regelmessig brukt for å utføre matematiske og statistiske beregninger i python. NumPy inkluderer mange praktiske funksjoner som hjelper å kalkulere og manipulere store mengder sata, mens Pandas kan være nyttig for manipulering av data filer som CSV og JSON filer ved å filtrere og manipulere data inn på for eksempel DataFrames. Fordi Numpy er lett integrert med andre libraries, som Pandas, jobber de bra sammen. For eksempel ved å formatere og filtrere ut informasjon med Pandas, og utføre beregninger som gjennomsnitt, median og standardavvik med NumPy. \n",
    "    \n",
    "    - Statistiske mål som gjennomsnitt, median og standardavvik kan gi en mer nøyaktig oversikt over dataene du har. Dette kan så brukes for å plotte ut nye modeller, samt hjelpe i tilfelle det vil oppstå skjevheter. Et eksempel er mangel på data, hvilket vil så kunne maskeres ved å bruke disse verdiene innenfor et visst område.\n",
    "\n",
    "2. Kan du gi et eksempel på hvordan du vil implementere en enkel statistisk analyse for å undersøke sammenhengen mellom to variabler i datasettet?\n",
    "    - Vi har valgt å bruke Numpy for å kunne gjøre en enkel statistist analyse for å sammenligne tider. \n",
    "\n",
    "3. Hvordan planlegger du å håndtere eventuelle skjevheter i dataene under analysen, og hvilke metoder vil du bruke for å sikre at analysen er pålitelig?\n",
    "    - Til å begynne med så oppdaget vi at vår datainnhenting var begrenset og overskrev det som allerede ble printet ut i en ekstern fil. Derfor så måtte vi kunne fikse for å i det hele tatt kunne sørge for at filene ville hente inn informasjon på en riktig måte. Når dette så ble gjort, så ville vi oppdage at vi kanskje kunne ha glemte en dag, hvilket ville lede til en manglende kilde av informasjon i forhold til hvordan ble skrevet. Dette førte til at vi kunne trenge å finne gjennomsnittet av de forskjellige verdier på hver sin side av den manglende verdien i seg selv for å holde det så jevnt som mulig.\n",
    "\n",
    "4. Hvilke visualiseringer vil du lage for å støtte analysen din, og hvordan vil disse visualiseringene hjelpe deg med å formidle funnene dine?\n",
    "    - For denne analysen så vil det være interessant å bedrive Linear Regression. Den vil la oss kunne gjøre kjappe utregninger i forhold til verdier kunne brukes til å se en generell sammenheng, om det skulle være noe blant de verdiene vi har valgt å se på.\n",
    "\n",
    "/Oppgave 5\n",
    "1. Hvilke spesifikke typer visualiseringer planlegger du å lage for å representere endringer i luftkvalitet og temperaturdata, og hvorfor valgte du disse?\n",
    "    - Et spotkart er det første som dukker opp, hvor en først vil se hvor de forskjellige verdiene ligger. Videre så vil\n",
    "2. Hvordan kan Matplotlib og Seaborn brukes til å forbedre forståelsen av de analyserte dataene, og hvilke funksjoner i disse bibliotekene vil være mest nyttige?\n",
    "    - Matplotlib er et bibliotek med allerede eksisterende matteformler for å kunne visualisere data. Når en så setter dette sammen med allerede eksisterende datasett, vil en kunne vise verdiene som grafer av forskjellige slag. Seaborn er så et bibliotek som brukes under Matplotlib, hvilket har en utvidet mengde funksjoner for å kunne skape forbedre grafiske fremvisninger.\n",
    "3. Hvordan vil du håndtere og visualisere manglende data i grafene dine for å sikre at de fortsatt er informative?\n",
    "    - Ved å observere de manglende aspektene av listene, så kan vi heller se på tidligere gjorte analyser. Hva vil standardavviket kunne være og hvordan vil dette hjelpe oss med å regne ut et gjennomsnitt. Alternativt så kan en bruke den sist gitte informasjonen og beholde den fram til ny informasjon kommer. Dette vil si at hvis en temperaturmåling har verdiene \"13, 14, NAN, NAN, 13\" så vil dette lede til \"13, 14, 14, 14, 13\". Dette kalles \n",
    "4. Kan du beskrive prosessen for å lage interaktive visualiseringer med Widgets, Plotly eller Bokeh, og hvilke fordeler dette kan gi i forhold til statiske visualiseringer?\n",
    "5. Hvordan vil du evaluere effektiviteten av visualiseringene dine i å formidle de viktigste funnene fra dataanalysen til et bredere publikum?\n",
    "\n",
    "/Oppgave 6\n",
    "1. Lag minst tre forskjellige typer visualiseringer (f.eks. linjediagrammer, søylediagrammer og scatterplots) for å representere endringer i luftkvalitet og temperaturdata over tid. Forklar valget av visualiseringstype for hver graf.\n",
    "2. Implementer visualiseringer ved hjelp av Matplotlib og Seaborn. Inkluder tilpassede akser, titler, og fargepaletter for å forbedre lesbarheten og estetikk.\n",
    "3. Demonstrer hvordan manglende data håndteres i visualiseringene. Lag en graf som viser hvordan manglende verdier påvirker datatrender, og diskuter hvordan dette kan påvirke tolkningen av dataene.\n",
    "4. Skriv en kort evaluering av de utviklede visualiseringene. Diskuter hvilke visualiseringer som var mest effektive for å formidle informasjon, og hvorfor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a53e7",
   "metadata": {},
   "source": [
    "/Oppgave 7\n",
    "\n",
    "Skriv et refleksjonsnotat (maks 800 ord) etter gjennomføringen av prosjektet. Denne skal inneholde viktige punkter som gir innsikt i deres læringsprosess, erfaringer og vurderinger av prosjektet.\n",
    "\n",
    "*Vurderingskriterier:*\n",
    "\n",
    "- Refleksjoner over hva du har lært om datainnsamling, databehandling, dataanalyse og visualisering.\n",
    "- Beskrivelse av nye ferdigheter som ble tilegnet, for eksempel bruk av spesifikke biblioteker (Pandas, NumPy, Matplotlib, etc.) og programmeringskonsepter.\n",
    "- Identifisering av spesifikke utfordringer som oppstod under prosjektet, for eksempel problemer med datakvalitet, håndtering av manglende verdier, eller tekniske problemer med API-er.\n",
    "- Refleksjoner over samarbeidet i gruppen, inkludert hvordan oppgaver ble fordelt og hvordan kommunikasjonen fungerte.\n",
    "- Vurdering av de endelige resultatene, inkludert kvaliteten på visualiseringene og analysene.\n",
    "- Ideer til hvordan prosjektet kan forbedres i fremtiden, både i forhold til tekniske aspekter og prosjektledelse.\n",
    "- Mulige retninger for videre forskning eller utvikling basert på erfaringene fra prosjektet.\n",
    "- Oppsummering av de viktigste læringspunktene og hvordan prosjektet har bidratt til studentenes forståelse av datavitenskap og miljøstudier.\n",
    "- Personlige tanker om hvordan erfaringene fra prosjektet kan anvendes i fremtidige studier eller yrkesliv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
