{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef03b76",
   "metadata": {},
   "source": [
    "# Miljødataanalyse - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fb84f",
   "metadata": {},
   "source": [
    "Mappe Del 1 Spørsmål\n",
    "\n",
    "////\n",
    "Describes the data directory and datasets\n",
    "\n",
    "#Potensielle API-kjelder\n",
    "- Google dataset search\n",
    "- Statistisk sentralbyrå\n",
    "- eea.europa //Net Greenhouse Gass Emmissions fra EEA\n",
    "    https://www.eea.europa.eu/en/datahub/featured-data/statistical-data/datahubitem-view/d22b842a-53f7-4c63-aa94-74d5fa1f4d40?activeAccordion=1093723\n",
    "- ssb //Forsurende gasser, forurensing, etc\n",
    "    https://www.ssb.no/statbank/table/08941/tableViewLayout1/\n",
    "- api.met.no\n",
    "- open-meteo.com\n",
    "    Denne virker potensiellt velldig bra...\n",
    "\n",
    "\n",
    "/Mappe_del_1, Oppgave 2/\n",
    "1. Hvilke åpne datakilder er identifisert som relevante for miljødata, og hva er kriteriene (f.eks. kildeautoritet, datakvalitet, tilgjengelighet, brukervennlighet osv.) for å vurdere deres pålitelighet og kvalitet?\n",
    "    - Vi har valgt å bruke enten Openweathermap sin API for å samle inn miljø-data, samt historisk data, eller open-meteo sine API-er. Noen API-er fra Openweathermap er utilgjengelige uten å betale. \n",
    "    - For å vurdere deres pålitelighet og kvalitet har vi sett på hvilke kilder desse sidene bruker for å samle sin informasjon. Videre har vi forsøkt å bruke real-time tester for å se om de samsvarer med områdene vi søker på i virkeligheten. Dette var videre lagt fram som et forslag av Professor Rouhani.\n",
    "2. Hvilke teknikker (f.eks. håndtering av CSV-filer, JSON-data) er valgt å bruke for å lese inn dataene, og hvordan påvirker disse valgene datakvaliteten og prosessen videre?\n",
    "    - API-ene fra Openweathermap bruker JSON format, men jeg er usikker på om open-meteo gjør det samme. Deres data kunne bli lastet ned fra terminal.\n",
    "    - Vi har valgt å bruke Visual Studio Code for å arbeide med jupyter-book teknologi\n",
    "3. Dersom det er brukt API-er, hvilke spesifikke API-er er valgt å bruke, og hva er de viktigste dataene som kan hentes fra disse kildene?\n",
    "    - API-er fra Openweathermap som omhandler trender innenfor hvordan været har endret seg over tiden. Dette ligger sterkt med tanke på økende temperatur og sterkere vindforhold.\n",
    "    - API-er fra Open-meteo vier og trender innenfor hvordan været har endret seg over tid. De har og en stor mengde med mulig statistik å kalle på. Fra regn, til temperatur, til jord foktighet. \n",
    "\n",
    "API Kjeldene\n",
    "/Fra Openweathermap\n",
    "- https://openweathermap.org/api/statistics-api\n",
    "/Fra Open-meteo\n",
    "- https://open-meteo.com/en/docs/historical-forecast-api\n",
    "\n",
    "/Mappe_Del_1, Oppgave 3/\n",
    "\n",
    "1. Hvilke metoder vil du bruke for å identifisere og håndtere manglende verdier i datasettet?\n",
    "    - For å kunne finne manglende verdier så vil dette kunne løses primært sett på to måter Først gjør en en test for å se om det finnes mangler. Om det mangler for mange verdier, så fjerner vi dem. Alternativt så kan vi fylle den inn selv, via flere forskjeller metoder. Eksempeltvis kan en gjøre det via en enkel count funksjon, alternativt ved å finne median-verdiene for de verdiene som mangler, basert på hva som er før og etter. \n",
    "2. Kan du gi et eksempel på hvordan du vil bruke list comprehensions for å manipulere dataene?\n",
    "    -  Et eksempel som gitt over kan være via grafer. Når en først finner en mangel, så vil det umiddelbart være tre muligheter. De vil så være basert på fillna, hvor en har en forward fill som gir deg samme verdi som den siste nevnte før det var mangel. Den andre versjonen er en backwards fill hvor den så går til neste tall som blir git og fyller den inn baklengs. Den tredje versjonen er median fill, hvor en bruker en tar median-verdi for av hva som kommer etter og før de manglende verdiene for å gi en overgang.\n",
    "3. Hvordan kan Pandas SQL (sqldf) forbedre datamanipuleringen sammenlignet med tradisjonelle Pandas-operasjoner?\n",
    "    - Ved større datasett så vil Pandas SQL sørge for en mer nøyaktig framstilling for videre analyse.\n",
    "4. Hvilke spesifikke uregelmessigheter i dataene forventer du å møte, og hvordan planlegger du å håndtere dem?\n",
    "    - I denne oppgaven planlegges det å møte manglende verdier, hvorav målet vil være å kunne finne ut hvor mange verdier som mangler av det totale datasettet. Om det skulle være for mange verdier, så vil disse droppes for å kunne unngå unøyaktigheter. Er de en påpasselig mengde, så vil vi gjøre en median-fill for å kunne sørge for en så gjennsnittlig måling som mulig. \n",
    "\n",
    "\n",
    "/Mappe del 2\n",
    "//Oppgave 4\n",
    "1. Hvordan kan du bruke NumPy og Pandas til å beregne gjennomsnitt, median og standardavvik for de innsamlede dataene, og hvorfor er disse statistiske målene viktige?\n",
    "    Numpy og Pandas er regelmessig brukt for å utføre matematiske og statistiske beregninger i python. NumPy inkluderer mange praktiske funksjoner som hjelper å kalkulere og manipulere store mengder sata, mens Pandas kan være nyttig for manipulering av data filer som CSV og JSON filer ved å filtrere og manipulere data inn på for eksempel DataFrames. Fordi Numpy er lett integrert med anndre libraries, som Pandas, jobber de bra sammen. For eksempel ved å formatere og filtrere ut informasjon med Pandas, og utføre beregninger som gjennomsnitt, median og standardavvik med NumPy. \n",
    "    \n",
    "    Statistiske mål som gjennomsnitt, median og standardavvik kan gi en mer nøyaktig oversikt over dataene du har. ---\n",
    "\n",
    "2. Kan du gi et eksempel på hvordan du vil implementere en enkel statistisk analyse for å undersøke sammenhengen mellom to variabler i datasettet?\n",
    "\n",
    "\n",
    "3. Hvordan planlegger du å håndtere eventuelle skjevheter i dataene under analysen, og hvilke metoder vil du bruke for å sikre at analysen er pålitelig?\n",
    "    - Det finnes flere måter å takle skjevheter under analysen, avhengig av skjevheten som oppdages. Den første tanken vi har angående manglende data\n",
    "\n",
    "4. Hvilke visualiseringer vil du lage for å støtte analysen din, og hvordan vil disse visualiseringene hjelpe deg med å formidle funnene dine?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddb19c",
   "metadata": {},
   "source": [
    "Mappe Del 2 Spørsmål"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
