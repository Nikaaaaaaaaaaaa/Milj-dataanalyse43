{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef03b76",
   "metadata": {},
   "source": [
    "# Miljødataanalyse - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fb84f",
   "metadata": {},
   "source": [
    "Mappe Del 1 Spørsmål\n",
    "/Oppgave 2\n",
    "1. Hvilke åpne datakilder er identifisert som relevante for miljødata, og hva er kriteriene (f.eks. kildeautoritet, datakvalitet, tilgjengelighet, brukervennlighet osv.) for å vurdere deres pålitelighet og kvalitet?\n",
    "    - Vi har valgt å bruke enten Openweathermap sin API for å samle inn miljø-data, samt historisk data, eller open-meteo sine API-er. Noen API-er fra Openweathermap er utilgjengelige uten å betale. \n",
    "    - For å vurdere deres pålitelighet og kvalitet har vi sett på hvilke kilder desse sidene bruker for å samle sin informasjon. Videre har vi forsøkt å bruke real-time tester for å se om de samsvarer med områdene vi søker på i virkeligheten. Dette var videre lagt fram som et forslag av Professor Rouhani.\n",
    "2. Hvilke teknikker (f.eks. håndtering av CSV-filer, JSON-data) er valgt å bruke for å lese inn dataene, og hvordan påvirker disse valgene datakvaliteten og prosessen videre?\n",
    "    - API-ene fra Openweathermap bruker JSON format, men jeg er usikker på om open-meteo gjør det samme. Deres data kunne bli lastet ned fra terminal.\n",
    "    - Vi har valgt å bruke Visual Studio Code for å arbeide med jupyter-book teknologi\n",
    "3. Dersom det er brukt API-er, hvilke spesifikke API-er er valgt å bruke, og hva er de viktigste dataene som kan hentes fra disse kildene?\n",
    "    - API-er fra Openweathermap som omhandler trender innenfor hvordan været har endret seg over tiden. Dette ligger sterkt med tanke på økende temperatur og sterkere vindforhold.\n",
    "    - API-er fra Open-meteo vier og trender innenfor hvordan været har endret seg over tid. De har og en stor mengde med mulig statistik å kalle på. Fra regn, til temperatur, til jord foktighet. \n",
    "\n",
    "/Oppgave 3\n",
    "1. Hvilke metoder vil du bruke for å identifisere og håndtere manglende verdier i datasettet?\n",
    "    - For å kunne finne manglende verdier så vil dette kunne løses primært sett på to måter Først gjør en en test for å se om det finnes mangler. Om det mangler for mange verdier, så fjerner vi dem. Alternativt så kan vi fylle den inn selv, via flere forskjeller metoder. Eksempeltvis kan en gjøre det via en enkel count funksjon, alternativt ved å finne median-verdiene for de verdiene som mangler, basert på hva som er før og etter. \n",
    "2. Kan du gi et eksempel på hvordan du vil bruke list comprehensions for å manipulere dataene?\n",
    "    -  Et eksempel som gitt over kan være via grafer. Når en først finner en mangel, så vil det umiddelbart være tre muligheter. De vil så være basert på fillna, hvor en har en forward fill som gir deg samme verdi som den siste nevnte før det var mangel. Den andre versjonen er en backwards fill hvor den så går til neste tall som blir git og fyller den inn baklengs. Den tredje versjonen er median fill, hvor en bruker en tar median-verdi for av hva som kommer etter og før de manglende verdiene for å gi en overgang.\n",
    "3. Hvordan kan Pandas SQL (sqldf) forbedre datamanipuleringen sammenlignet med tradisjonelle Pandas-operasjoner?\n",
    "    - Ved større datasett så vil Pandas SQL sørge for en mer nøyaktig framstilling for videre analyse.\n",
    "4. Hvilke spesifikke uregelmessigheter i dataene forventer du å møte, og hvordan planlegger du å håndtere dem?\n",
    "    - I denne oppgaven planlegges det å møte manglende verdier, hvorav målet vil være å kunne finne ut hvor mange verdier som mangler av det totale datasettet. Om det skulle være for mange verdier, så vil disse droppes for å kunne unngå unøyaktigheter. Er de en påpasselig mengde, så vil vi gjøre en median-fill for å kunne sørge for en så gjennsnittlig måling som mulig. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddb19c",
   "metadata": {},
   "source": [
    "Mappe Del 2 Spørsmål"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8fbd4",
   "metadata": {},
   "source": [
    "/Oppgave 4\n",
    "1. Hvordan kan du bruke NumPy og Pandas til å beregne gjennomsnitt, median og standardavvik for de innsamlede dataene, og hvorfor er disse statistiske målene viktige?\n",
    "    - Numpy og Pandas er regelmessig brukt for å utføre matematiske og statistiske beregninger i python. NumPy inkluderer mange praktiske funksjoner som hjelper å kalkulere og manipulere store mengder sata, mens Pandas kan være nyttig for manipulering av data filer som CSV og JSON filer ved å filtrere og manipulere data inn på for eksempel DataFrames. Fordi Numpy er lett integrert med andre libraries, som Pandas, jobber de bra sammen. For eksempel ved å formatere og filtrere ut informasjon med Pandas, og utføre beregninger som gjennomsnitt, median og standardavvik med NumPy. \n",
    "    \n",
    "    - Statistiske mål som gjennomsnitt, median og standardavvik kan gi en mer nøyaktig oversikt over dataene du har. Dette kan så brukes for å plotte ut nye modeller, samt hjelpe i tilfelle det vil oppstå skjevheter. Et eksempel er mangel på data, hvilket vil så kunne maskeres ved å bruke disse verdiene innenfor et visst område.\n",
    "\n",
    "2. Kan du gi et eksempel på hvordan du vil implementere en enkel statistisk analyse for å undersøke sammenhengen mellom to variabler i datasettet?\n",
    "    - Vi har valgt å bruke Numpy for å kunne gjøre en enkel statistisk analyse for å sammenligne tider på døgnet samt temperatur.Dette gjorde vi ved å sammenligne dag og natt, for å så kunne se hvordan det kunne oppstå mønster der. Det vi fant ut, var at det var lavere temperatur om natten.\n",
    "\n",
    "3. Hvordan planlegger du å håndtere eventuelle skjevheter i dataene under analysen, og hvilke metoder vil du bruke for å sikre at analysen er pålitelig?\n",
    "    - Til å begynne med så oppdaget vi at vår datainnhenting var begrenset og overskrev det som allerede ble printet ut i en ekstern fil. Derfor så måtte vi kunne fikse for å i det hele tatt kunne sørge for at filene ville hente inn informasjon på en riktig måte. Når dette så ble gjort, så ville vi oppdage at vi kanskje kunne ha glemte en dag, hvilket ville lede til en manglende kilde av informasjon i forhold til hvordan ble skrevet. Dette førte til at vi kunne trenge å finne gjennomsnittet av de forskjellige verdier på hver sin side av den manglende verdien i seg selv for å holde det så jevnt som mulig.\n",
    "\n",
    "4. Hvilke visualiseringer vil du lage for å støtte analysen din, og hvordan vil disse visualiseringene hjelpe deg med å formidle funnene dine?\n",
    "    - For denne analysen så vil det være interessant å bruke et scatterplot. Den vil la oss kunne se fort over verdiene som brukes, for å se hvordan situasjonen stemmer. En kan videre bruke et søylediagram for å vise vekst og senkelse over tiden som passerer.\n",
    "\n",
    "/Oppgave 5\n",
    "1. Hvilke spesifikke typer visualiseringer planlegger du å lage for å representere endringer i luftkvalitet og temperaturdata, og hvorfor valgte du disse?\n",
    "    - Et scatterplot er det første som dukker opp, hvor en først vil se hvor de forskjellige verdiene ligger. Videre så vil søylediagrammer vise klar informasjon i en sammenlingningsprosess, samt være enklere å lese.\n",
    "2. Hvordan kan Matplotlib og Seaborn brukes til å forbedre forståelsen av de analyserte dataene, og hvilke funksjoner i disse bibliotekene vil være mest nyttige?\n",
    "    - Matplotlib er et bibliotek med allerede eksisterende matteformler for å kunne visualisere data. Når en så setter dette sammen med allerede eksisterende datasett, vil en kunne vise verdiene som grafer av forskjellige slag. Seaborn er så et bibliotek som brukes under Matplotlib, hvilket har en utvidet mengde funksjoner for å kunne skape forbedre grafiske fremvisninger.\n",
    "3. Hvordan vil du håndtere og visualisere manglende data i grafene dine for å sikre at de fortsatt er informative?\n",
    "    - For å kunne forsikre at grafene fortsatt skal være informative så må det finnes en måte å anskaffe disse verdiene. En kan enten replikere det sist gitte tallet før verdiene forsvinner, eller det neste some dukker opp etter de manglende verdiene (forward fill og backwards fill, respektivt). En kan videre kjøre et gjennomsnitt for å kunne få en mer generell verdi basert på hele datasettet, samt linear som skape en klar og jevn linje mellom verdiene. Disse vil i alle tilfeller lede en polynomial graf til å skape heller rette linjer. Hvilket leder til det siste alternativet: polynomial, hvor en ser på forventet økning og fall, for så å sette dette inn i grafen. Krever mer arbeid.\n",
    "4. Kan du beskrive prosessen for å lage interaktive visualiseringer med Widgets, Plotly eller Bokeh, og hvilke fordeler dette kan gi i forhold til statiske visualiseringer?\n",
    "    - Vi har valgt å ikke skape interaktive visualiseringer, men heller fokusere på enkle visualiseringer med hensikt av å være fort forståelige.\n",
    "5. Hvordan vil du evaluere effektiviteten av visualiseringene dine i å formidle de viktigste funnene fra dataanalysen til et bredere publikum?\n",
    "    - Ved å skape disse visualiseringene vil vi kunne enklere formidle variasjonen av temperatur og luftfuktighet. Dette gjør større datasett mer lesbare via enkel grafikk. Til å begynne med så er linjediagramm nyttig for å vise endring over tid. Videre så vil gjennomsnittet la oss forstå hvordan det vil generellt bevege seg videre. \n",
    "    - Scatterplottet gir en innsikt inn i et annet felt, til den grad hvordan fuktighet og temperatur vil være sammenknyttet.\n",
    "    - Etter det har vi også en enkel graf for å vise tilfellet av manglende data som eksisterer før og uten rensing.\n",
    "/Oppgave 6\n",
    "1. Lag minst tre forskjellige typer visualiseringer (f.eks. linjediagrammer, søylediagrammer og scatterplots) for å representere endringer i luftkvalitet og temperaturdata over tid. Forklar valget av visualiseringstype for hver graf.\n",
    "    - Vi har først valgt å bruke en lineær regresjonsmodell for å vise temperaturforutsigelser for de neste 24 timer. Dette brukes for å måle hvordan den forutsagte og faktiske temperaturen kan variere.\n",
    "    - Vi har deretter valgt å lage en polynomial regresjonsmodell basert på den tidligere lineære modellen, hvor målet er å så kunne bruke disse verdiene over lengre tid.\n",
    "2. Implementer visualiseringer ved hjelp av Matplotlib og Seaborn. Inkluder tilpassede akser, titler, og fargepaletter for å forbedre lesbarheten og estetikk.\n",
    "    - Dette vises i Miljødataanalyse_kode.ipynb. Det ble har ansvarlig brukt forskjellige farger for å kunne best skilne mellom de forskjellige typer av verdier, samt hvordan det var klare punkter mellom hver verdi.\n",
    "3. Demonstrer hvordan manglende data håndteres i visualiseringene. Lag en graf som viser hvordan manglende verdier påvirker datatrender, og diskuter hvordan dette kan påvirke tolkningen av dataene.\n",
    "    - Det har vært lite data som har vært nødvendig å renses, hvorav denne grafen vil finnes i oppgave 5. Denne viser til hvor tilfellet har oppstått, og viser også hele historien rundt. Dette kan brukes til å vise en ærlig temperatur-forskjell, samt også skape en fortolket virkelighet angående hvordan temperaturen mest sannsynlig kan ha vært.\n",
    "4. Skriv en kort evaluering av de utviklede visualiseringene. Diskuter hvilke visualiseringer som var mest effektive for å formidle informasjon, og hvorfor. \n",
    "    - Den lineære regresjonen fungerer best for å skape en enkel forståelse av emnet og hvordan ting har endret seg, samt for å gjøre grunnlegende matte. Dette gjør at den ikke er den mest effektive, men heller skaper et godt grunnlag.\n",
    "    - Den polynomiale regresjonen skaper bedre forståelse for hvordan endringer vil oppstå over tid, hvilket vil lede til en mer effektiv graf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a53e7",
   "metadata": {},
   "source": [
    "/Oppgave 7\n",
    "\n",
    "Skriv et refleksjonsnotat (maks 800 ord) etter gjennomføringen av prosjektet. Denne skal inneholde viktige punkter som gir innsikt i deres læringsprosess, erfaringer og vurderinger av prosjektet.\n",
    "\n",
    "*Vurderingskriterier:*\n",
    "\n",
    "- Refleksjoner over hva du har lært om datainnsamling, databehandling, dataanalyse og visualisering.\n",
    "- Beskrivelse av nye ferdigheter som ble tilegnet, for eksempel bruk av spesifikke biblioteker (Pandas, NumPy, Matplotlib, etc.) og programmeringskonsepter.\n",
    "- Identifisering av spesifikke utfordringer som oppstod under prosjektet, for eksempel problemer med datakvalitet, håndtering av manglende verdier, eller tekniske problemer med API-er.\n",
    "    - Det oppsto problemer i forhold til Unittesting. Videre så var det problemer med datasettene vi fant, hvor det vi brukte i store deler av begynnelsen viste seg å være vanskelig å fortsette med, hvilket førte til en helomvending til et svakere, men fungerende, datasett. Dette kom av at den filen vi fikk ut av datasett 1 var en .pycache som ikke ville la seg lesee, i forhold til en .csv fil.\n",
    "- Refleksjoner over samarbeidet i gruppen, inkludert hvordan oppgaver ble fordelt og hvordan kommunikasjonen fungerte.\n",
    "- Vurdering av de endelige resultatene, inkludert kvaliteten på visualiseringene og analysene.\n",
    "- Ideer til hvordan prosjektet kan forbedres i fremtiden, både i forhold til tekniske aspekter og prosjektledelse.\n",
    "- Mulige retninger for videre forskning eller utvikling basert på erfaringene fra prosjektet.\n",
    "- Oppsummering av de viktigste læringspunktene og hvordan prosjektet har bidratt til studentenes forståelse av datavitenskap og miljøstudier.\n",
    "- Personlige tanker om hvordan erfaringene fra prosjektet kan anvendes i fremtidige studier eller yrkesliv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
